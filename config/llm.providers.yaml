# LLM Provider Configuration
# Defines available LLM providers, their endpoints, models, and default parameters

providers:
  anthropic:
    endpoint: https://api.anthropic.com/v1
    models:
      - name: claude-3-5-sonnet-20241022
        aliases: ["claude-3-5-sonnet", "sonnet"]
        context_window: 200000
        max_output: 8192
        cost_per_1k_input: 0.003
        cost_per_1k_output: 0.015
      - name: claude-3-5-haiku-20241022
        aliases: ["claude-3-5-haiku", "haiku"]
        context_window: 200000
        max_output: 8192
        cost_per_1k_input: 0.001
        cost_per_1k_output: 0.005
      - name: claude-3-opus-20240229
        aliases: ["claude-3-opus", "opus"]
        context_window: 200000
        max_output: 4096
        cost_per_1k_input: 0.015
        cost_per_1k_output: 0.075
    default_model: claude-3-5-sonnet-20241022
    default_params:
      temperature: 0.3
      max_tokens: 4000
      top_p: 0.95
      top_k: 40
      
  openai:
    endpoint: https://api.openai.com/v1
    models:
      - name: gpt-4o
        aliases: ["gpt4o"]
        context_window: 128000
        max_output: 16384
        cost_per_1k_input: 0.0025
        cost_per_1k_output: 0.01
      - name: gpt-4o-mini
        aliases: ["gpt4o-mini"]
        context_window: 128000
        max_output: 16384
        cost_per_1k_input: 0.00015
        cost_per_1k_output: 0.0006
      - name: gpt-4-turbo
        aliases: ["gpt4-turbo"]
        context_window: 128000
        max_output: 4096
        cost_per_1k_input: 0.01
        cost_per_1k_output: 0.03
      - name: o1-preview
        aliases: ["o1"]
        context_window: 128000
        max_output: 32768
        cost_per_1k_input: 0.015
        cost_per_1k_output: 0.06
      - name: o1-mini
        aliases: ["o1mini"]
        context_window: 128000
        max_output: 65536
        cost_per_1k_input: 0.003
        cost_per_1k_output: 0.012
    default_model: gpt-4o-mini
    default_params:
      temperature: 0.2
      max_tokens: 4000
      top_p: 1.0
      frequency_penalty: 0
      presence_penalty: 0
      
  google:
    endpoint: https://generativelanguage.googleapis.com/v1beta
    models:
      - name: gemini-1.5-pro-002
        aliases: ["gemini-pro", "gemini-1.5-pro"]
        context_window: 2000000
        max_output: 8192
        cost_per_1k_input: 0.00125
        cost_per_1k_output: 0.005
      - name: gemini-1.5-flash-002
        aliases: ["gemini-flash", "gemini-1.5-flash"]
        context_window: 1000000
        max_output: 8192
        cost_per_1k_input: 0.000075
        cost_per_1k_output: 0.0003
      - name: gemini-2.0-flash-exp
        aliases: ["gemini-2-flash"]
        context_window: 1000000
        max_output: 8192
        cost_per_1k_input: 0.00  # Experimental pricing
        cost_per_1k_output: 0.00
    default_model: gemini-1.5-pro-002
    default_params:
      temperature: 0.3
      max_output_tokens: 4000
      top_p: 0.95
      top_k: 40
      
  local:
    endpoint: http://localhost:11434/api  # Ollama default
    models:
      - name: llama3.2:70b
        context_window: 8192
        max_output: 4096
        cost_per_1k_input: 0.00
        cost_per_1k_output: 0.00
      - name: mixtral:8x7b
        context_window: 32768
        max_output: 4096
        cost_per_1k_input: 0.00
        cost_per_1k_output: 0.00
      - name: deepseek-coder-v2
        context_window: 128000
        max_output: 8192
        cost_per_1k_input: 0.00
        cost_per_1k_output: 0.00
    default_model: llama3.2:70b
    default_params:
      temperature: 0.2
      max_tokens: 2000
      
# Global settings
global:
  timeout_seconds: 60
  retry_on_rate_limit: true
  max_retries: 3
  enable_streaming: true
  enable_caching: true
  cache_ttl_seconds: 3600
